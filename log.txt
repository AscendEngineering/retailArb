
NOTES
__________________________________________________________________
-the idea
	-scrape pages for items
	-look at ebay and craigslist to see if there are items there that are selling for more
-ok so we are going to need to be able to scrape the page, lets see if python has some good scraping tools
-so we do need to set up access to the at home clusters, and then we need to make sure to set up those emails. from there we can do other stuff like the arbitrage machine
-what we should do is make a script that tries to connect to the machines locally and if that is not an option then we connect to shitech and specify the ports
-so it looks like I need to run this program a certain type of way, with their dumb command...whatever, ill try it but I wont liek it
-okay so we are going to need to figurte out how to go to next pages...well we might not be able to find the url...which is weird...
-ok so now that we have the links down and the title we can actually search google and get results back....oh but we also might want to integrate this into one of our scripts as well
-yeah lets output all that stuff to a file, then we can iterate through it
-damn this scrapy thing is kind of hard to work with tbh.
-ok cool, now we have this database and now I need to think about how I am going to handle actually writing this data and recalling it.....so...how are we going to organize this datab
    -we need an id field realistically. I do not want to have to rely on the
-so this is how it should work. well, lets see if we can scrape some sort of id off of the page to store in our db
    -there is an id that we can scrape
-ok now that we have that we can correctly add items into mongodb, since we actually have a unique id now
-haha well we also want to make sure that we are actually write to the database
-so now we need to look at things from the database, find them in google shopping, and then ... well are we going to search by image? because if that is the case then we
    need to figure out how to do that...it would definitely make things more accurate.....or we coud build out the rest of the loading from craigslist

-ok so now that we have that done we are going to want to sort by new and then stop processing when we reach a pid that we have processed already, since that means we
    have already processed all the rest, and it also means we can search through these every few minutes without raises suspicion
-we should flesh out the part of this where we get the price off of ebay. should we use google or ebay? we need to do this before we just load up our db with a ton of useless
    posts. because what good is that?
-for right now, for this morning, lets concentrate on making sure that if a post occurs in multiple searches we dont just stop the search if we have seen it before
-so we are not searching by image, or at least at this point. text seems to be the best way to search these so far.
-so we need to do a google search in the category shopping, then scrape the results

-so I bet most of our search returns will come from ebay, especially since they are used items. so I am wondering if we can just scrape ebay...worst comes to worst we can
    submit a form to the backend of google,and it should hit us with a html page with the needed info

-okay so ebay's api is a bit more advanced too since it is showing some tickets in the url that I am not completely familiar with, but
    we might be able to capture an outgoing form and see how it is formed....scrapy will definitely have some type of form api..right? hopefully
    -yeah it looks like we can capture the form...,fill out the properfields, then submit it back...either.
    -but, we are going to need to figure out exactly what form it is
    -okay so the scrapy form already seems to prepopulate the ones that we need, so we can just change the one that we need to....but I am unsure how I can point it at t
    he right form
    -so in the future we want this crawler to run immediatly after we find a new item.......but
-so lets do this...I bet we can download the picture, then use the text to search on ebay, download ebay photos, then compare the photos and take the price of the ones that match the most
-yeah we are going to need to download these fucking pictures.....fuck...this makes things a bit harder,b ut we need that accuracy
-lets go ahead and work on the downloading of the pictures.
-ok so at this second lets work on storing the query name as well as the filename
-ugh so are we going to straight use a module to download this photo and then store it in some place "images folder"
-SO we can probably make sure that no 2 photos are duplicated by checking to see if a file exists with that name in the images folder. that would probably
    be easiest
-alright so that seemed to work well...hopefully PIDs are not repeated, or else we are kind fucked
-okay so now we are going to need to implement the search query in the database
-so for right now it seems like we have something that will actually download everything that we need from craigslist, which is nice
-now we actually have to work on the ebay part of this

-I need to work out a way to get these ebay results, because we want the results, and the results that match fewer words.
-then we need to grab those....should we store all of them and then compare??
-well yeah what we should do is pull in all the entries.....and compare each one with the one in the db that we have....or does the compare method that
-we are going to have gouing to accept a url, because in that case we might not want to pull those photos in
-well hopefully the method can handle the url i
-good news is that the image comparison works...bad news is that we need to resize images to compare them ising ssim....which is going to be interesting figuring that ones
    out.
-what I seem to be seeing is that ssim measures the quality of the photo in relation to another photo, and I am not sure if that is what we want exactly. i think
    it was naive of me to think that this algorithm would just give me a score and say this is how similar these two images are. I need to maybe run a few different algorihtms
    and then get a score from that
-okay so lets create a suite of images that we can compare with each other and from there we can get what we are working with....oh, but they need to be the same size....,
and that is not going to work with ssim....so we need a module to resize. obviosuly we will resize to the middle.tbh both of them will have to be resized.. we will take the
smaller height and width, then resize both images to that size
-okay, so it looks like this heightened image comaprison method takea a bit of time
-okay, lets implement both and see which one is better with our test suite
-ok, looks good. I found the algorithm that I wanted. I didnt even need to implement both algorihtms
-so the next step is scrape ebay...which is going to be a pain in the ass
-well what I actually want to do is look up, see if there are any results, and if not run it through a text filter and then try again

-we should probably be able to load bullshit data onto the database...eh...whatveer
-now I am wondering whether we are actually/.....well we are definitely searching the mongo db.
-so we have 2 options....we can make these api requests that are designed by ebay...or we can use our own scrapy crawler to fill out these
forms and make the calls themselves......if we make the calls ourselves it is guaranteed to be free, we can see relevant items, and we can make sure to get the item image
-however, we might get a cease and assist, and it might not be as fast.....it might also not be as complex.
-I am leaning towards just using scrapy...plus in my heart I somehow realize taht I am going to habe to pay for something wiht ebay api....theres a reason that
    they are offering it..mmm i see. they are using it so that users can

-so now we have to make a whole new scraper for ebay is what I am hearing.....which won't be crazy hard...we just need to fill out the form

-ebay
    -go to the main page, grab the form, fill it out, and submit....I am not sure how
    -so we will use the from_response...but I am unsure how it will exactly know which form to fill out

-okay so we are sitting pretty. we need to figure out how to take each item and either store it or run it through comparison...
-either that...or we could do a poor mans way of doing this and just take the prices...average them out and then from there we can figure out what we really need to do
    -shit thats actually not a bad idea..we can just run statistics on the prices...find out what the price is...factor out any outliers
    -but there are two parts to this. because some results will end up yielding nothing on ebay. so we do need to implement a search term modifier. probably present in
    the ebay crawler itself. so we need to implement something that will get rid of common words like "the"...idk about getting rid of numbers. dates might be
    something that we want to keep....hmmm. lets worry about the text modifier later and worry about collecting those statistics now.
    -we do not want to hold onto these statistics. all we need to do is collect and report. this also means that we can stop downloading the images from craigslist..but
    if I have to be honest with you I might want those for the reports....hmmm...we also need to move some other scripts over to our other servers, but that
    is besides the point.
    -what we really need to do is map out how we are going to report whether there is a match or no. what we can do is pass in the price of the item
    into the ebay crawler. then we run statistics and find out if it is higher or lower.....well....we can pass in the pid from craigslist...look it up in mongodb
    get the price from there. this will make things a lot easier when we want to output whether something is more expensive or not. because then we have all
    of the item info in the ebayCrawler itself. then...if we want to add items later we can just pass in the pid from the perspective website to get it's info.
    this seems to make the most sense right now...so we will pass in the pid and go from there
-so we just need to get the numbers until the related search section


TODO
__________________________________________________________________
-scrape
	-ebay
	-craigslist
	-facebook marketplace
-someone has done this before, so lets see if we can imitate
	-look up the amazon seller app
	-fullfillment by amazon revenue calculator
-sign up for amazon seller account
-learn more about how retail arbitrage works
	-its pretty simple, but there are some rules that we should follow. but i think the main thing is just diving in
-target the program to look at garage and moving sales too, these people are motivated to leave
-target posts that say they need quick cash
-update collectUrls method
-create a nosql database
-create a script to handle mongodb restart
    -sudo service mongod status/restart/start/stop
-connect the scraper to our db so that we can immediatly see if we have seen an item before
-alternative to directly above, see if we can scan for items that were posted since the last time that we scaned
    -we can select posts that were made in the last day, lets make cmd line argument that we can either do full scan or just the past day, then we can run this once per Day
    -the only problem with this is that we will not get automatic updates. if we want autoatic updates we need to log the entries into mongodb on the fly
-create some type of process where we go through and delete old posts that are not longer there
-what if an item shows up in 2 different searches, but we stop at that item since the way we have it set up is that we stop scanning when we have already seen an item
    that is in our database. we need to store the search query that we used as well so we can know whether we should stop or not. so we would look up an item's pid
    and the search query used. if the pid matches but the search query does not then we keep scanning, if both match then we stop scanning
-also, we should make sure that an item's PID is unique, because the url that is being shown includes the PID and the name of the item, so that is suspicious
-make it so that when something new gets posted we automatically look up if there is an arbitrage opportunity
-add it shipping costs
-figure out a way that we can also search using certain things from the descriptions
-we are going to need to make sure that ebay doesn't pick up on us trying to scrape its website...so we should get a few different api keys
-we also might need to store the image name for the photos, since they might be a different file format.....should we worry about this later???
    -probably not...alright thats done..now we need to make sure that if this is not the current query then we keep scanning, but if it is then we stop
-make sure that it actually works that when we see a duplicate post in a different query that we skip it if we have seen it but different query, but stop all together if
    it is the same query
-if the first ebay search does not return anything
    -strip away all the unecessary search words when we are combing through ebay. numbers, filler words, anything that is not a brand name or product description, then use
    that to search.
    -then we should scan through the pictures, and if there is one that gives us a match above a certain point then we have a match
-we should also create some test cases
-find a way to make 2 images have the same dimensions
-we should probably add some error checking in here
-filter out "wanted" posts



DONE
__________________________________________________________________
-write a script that automatically connects us to the local version of our servers, and if that fails connect us remotely
	-nope, it takes way too long for ssh to timeout for a local port, so we are just going to know when we need to connect remotely
	-nevermind, I will just set up a timeout variable...I wonder if the router is smart enough to realize what its ip address is and route the traffic logically...idk
	-yeah this worked
-set up remote connection abilities to server1 and server2
	-make sure duckdns gets updates
	-we dont have to worry about this as long as one of the PC's updates it, and I am pretty sure server2 does this
-make sure we are not storing duplicate posts
-we need to push this stuff to github
-make sure that we are not downloading the same photo twice




IMPROVE
__________________________________________________________________
-the biggest way that we can improve this right now is to improve the search between sites.
    -we need the price between sites to be more accurate





RESOURCES
__________________________________________________________________
-image comparison good post
    https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/
-good video series on image comparisons
    -https://www.youtube.com/watch?v=ND5vGDNvN0s&list=PL6Yc5OUgcoTlQuAdhtnByty15Ea9-cQly
    -using ssim
        -faster, not as accurate, need the image to be the same size
    -using feature similarities
        -slower, more accurate, images do not need to be the same size
-mongodb commands
    -mongo: interactive shell
    -sudo service mongod status/restart/start/stop: controls the mongo service
    -db.collection_name.find().pretty(): print out all the things in a collection
    -db.collection.remove({}): removes everything from a collection
-My database: arbitragedb
    -collections
        craigslist
-here is the list of marketplaces that we can sell to
	-Amazon, eBay, Jet, Walmart.com, Etsy, Craigslist, Facebook Marketplace, OfferUp, Let Go
	-Amazon seems to be the best
	-even more sites: https://wellkeptwallet.com/best-apps-to-sell-stuff/
-arbitraging items
	-https://onlinesellingexperiment.com/retail-arbitrage-2/
		-set minimum earnings on item (guy has his at 3)
		-make sure your return on investment is at least 50%
		-they also recommend sticking to a minimum of 6 units of something when purchasing
-Some profitable items to look for on Craigslist:
    Littlest Pet Shop collections
    Disney Halloween costumes
    Plush stuffed animals
    Direct sales companies distributor going out of business (Pampered Chef, Shaklee, Melaleuca, Arbonne, Scentsy, etc.)
    Cast iron cookware
    Pottery Barn items
    Craft supplies
    electronics
    cell phones
    power tools
    computers
    Mugs
    Matchbox cars
    Christmas ornaments (Christopher Radko, Waterford, Lenox)
    Vitamin blenders, KitchenAid small appliances
    Designer handbags (could be fake)
    Electronics (make sure they work properly and all parts are included)
    High-end sunglasses like Ferragamo, RayBan, Prada, and Oakley as they could be fake
    Autographed items (make sure it is authenticated)





{ "_id" : ObjectId("5c6a0aca4dc5050d7efa9ebd"), "pid" : "6817367571", "title" : "Bears Superbowl XX Stein Mug", "price" : 15, "query" : "mug", "format" : "jpg", "link" : "https://chicago.craigslist.org/wcl/clt/d/bears-superbowl-xx-stein-mug/6817367571.html" }
